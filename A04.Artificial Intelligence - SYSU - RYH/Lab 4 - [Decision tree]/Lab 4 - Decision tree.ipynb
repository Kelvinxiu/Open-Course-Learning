{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.014108Z",
     "start_time": "2017-11-17T15:34:22.553156Z"
    },
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import pdir as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集处理\n",
    "\n",
    "本次数据分为 train.csv 和 test.csv。每个文件有10列，前9列为特征 （都为离散型），最后一列是标签（只有1和-1两种取值）。\n",
    "\n",
    "## 数据集读取函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.035023Z",
     "start_time": "2017-11-17T15:34:23.014108Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet(filePath):\n",
    "    ''' 数据集读取函数'''\n",
    "    data, label = [], []\n",
    "    # 读取数据集\n",
    "    with open(filePath) as f:\n",
    "        for line in f.readlines():\n",
    "            temp = line.strip().split(\",\")\n",
    "            data.append([float(i) for i in temp[:-1]])\n",
    "            if temp[-1] != '?':\n",
    "                temp[-1] = float(temp[-1])\n",
    "            label.append(temp[-1])\n",
    "    ##### 输出数据集相关信息 ##########\n",
    "    print(\"data dimension of dataset：\", len(data[0]))\n",
    "    print(\"number of sample in data :\", len(data))\n",
    "    print(\"label frequency:\", dict(Counter(label)))\n",
    "    ##### 输出数据集相关信息 ##########\n",
    "    return np.array(data), np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取原始训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.291033Z",
     "start_time": "2017-11-17T15:34:23.035023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 787\n",
      "label frequency: {1.0: 323, -1.0: 464}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 23.,   1.,   2., ...,   2.,   0.,   0.],\n",
       "       [ 36.,   1.,   1., ...,   2.,   3.,   0.],\n",
       "       [ 33.,   1.,   3., ...,   2.,   3.,   0.],\n",
       "       ..., \n",
       "       [ 49.,   0.,   1., ...,   1.,   1.,   1.],\n",
       "       [ 21.,   1.,   3., ...,   2.,   1.,   0.],\n",
       "       [ 31.,   3.,   3., ...,   1.,   3.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet_origin, trainSet_label_origin = loadDataSet('.\\\\data\\\\train.csv')\n",
    "trainSet_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.451041Z",
     "start_time": "2017-11-17T15:34:23.291033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 300\n",
      "label frequency: {'?': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 32.,   3.,   3., ...,   0.,   3.,   0.],\n",
       "       [ 24.,   1.,   2., ...,   1.,   2.,   0.],\n",
       "       [ 32.,   3.,   3., ...,   0.,   2.,   0.],\n",
       "       ..., \n",
       "       [ 43.,   2.,   2., ...,   2.,   2.,   0.],\n",
       "       [ 21.,   1.,   1., ...,   3.,   2.,   0.],\n",
       "       [ 30.,   2.,   3., ...,   2.,   2.,   0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet, testSet_lable = loadDataSet('.\\\\data\\\\test.csv')\n",
    "testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从训练集中划分验证集\n",
    "\n",
    "### 随机划分\n",
    "\n",
    "典型的从训练集中划分验证集的方法是：划分训练集中的30%为验证集，划分过程采用随机选取的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.663055Z",
     "start_time": "2017-11-17T15:34:23.451041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分后的验证集和训练集的维度： (236, 9) (551, 9)\n"
     ]
    }
   ],
   "source": [
    "def splitTrainSet2ValidateSet(trainSet_, trainSet_label_, rate=0.3):\n",
    "    '''从训练集中划分验证集'''\n",
    "    #整合数据集和对应标签在同一个数组上，方便后续划分\n",
    "    allData = np.column_stack((trainSet_, trainSet_label_))\n",
    "    #随机打乱数据集\n",
    "    np.random.shuffle(allData)\n",
    "    #得到要划分的验证集的样本个数\n",
    "    splitNum = int(allData.shape[0]*rate)\n",
    "    #划分数据集\n",
    "    validateSet_ = allData[:splitNum, :-1]\n",
    "    validateSet_label_ = allData[:splitNum, -1]\n",
    "    trainSet_new = allData[splitNum:, :-1]\n",
    "    trainSet_label_new = allData[splitNum:, -1]\n",
    "    return trainSet_new, trainSet_label_new, validateSet_, validateSet_label_\n",
    "\n",
    "temp = splitTrainSet2ValidateSet(trainSet_origin, trainSet_label_origin)\n",
    "trainSet, trainSet_label, validateSet, validateSet_label = temp\n",
    "print('划分后的验证集和训练集的维度：', validateSet.shape, trainSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动划分\n",
    "\n",
    "有一些同学采用手动划分的方式来划分数据集，为了更好地对比结果，这里也使用手动划分的方式来得到验证集。\n",
    "\n",
    "具体划分方法为：在原始训练集的基础上取前100个和后100个样本作为验证集。\n",
    "\n",
    "划分后验证集和训练集各自保存在`validation_1.csv`和`train_1.csv`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.863068Z",
     "start_time": "2017-11-17T15:34:23.663055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 587\n",
      "label frequency: {1.0: 223, -1.0: 364}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 28.,   3.,   3., ...,   0.,   3.,   0.],\n",
       "       [ 36.,   3.,   3., ...,   1.,   3.,   0.],\n",
       "       [ 47.,   3.,   3., ...,   0.,   2.,   0.],\n",
       "       ..., \n",
       "       [ 22.,   3.,   3., ...,   1.,   0.,   1.],\n",
       "       [ 36.,   1.,   2., ...,   1.,   2.,   0.],\n",
       "       [ 47.,   1.,   1., ...,   1.,   1.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 200\n",
      "label frequency: {1.0: 100, -1.0: 100}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 23.,   1.,   2., ...,   2.,   0.,   0.],\n",
       "       [ 36.,   1.,   1., ...,   2.,   3.,   0.],\n",
       "       [ 33.,   1.,   3., ...,   2.,   3.,   0.],\n",
       "       ..., \n",
       "       [ 49.,   0.,   1., ...,   1.,   1.,   1.],\n",
       "       [ 21.,   1.,   3., ...,   2.,   1.,   0.],\n",
       "       [ 31.,   3.,   3., ...,   1.,   3.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet2, trainSet2_label = loadDataSet('.\\\\data\\\\train_1.csv')\n",
    "trainSet2\n",
    "validateSet2, validateSet2_label = loadDataSet('.\\\\data\\\\validation_1.csv')\n",
    "validateSet2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选取\n",
    "\n",
    "## 信息增益和信息增益率\n",
    "\n",
    "### 实现原理\n",
    "\n",
    "#### 信息增益\n",
    "\n",
    "计算步骤如下：\n",
    "\n",
    "- 1.计算数据集D的熵：$H(D)=-\\sum_{d\\in D}p(d)*log[p(d)]$\n",
    "\n",
    "  - $p(d)$为标签d出现的概率\n",
    "\n",
    "\n",
    "- 2.计算特征A相对于数据集D的条件熵：$H(D|A)=\\sum_{a \\in A}p(a)H(D|A=a)$\n",
    "  \n",
    "  - $H(D|A=a)$为特征A取值为a时D中对应的子数据集的熵\n",
    "\n",
    "\n",
    "- 3.计算信息增益：$Gain(D,A)=H(D)-H(D|A)$\n",
    "\n",
    "\n",
    "在选用信息增益来为决策树选取划分结点时，**信息增益最大**的特征将被视为当前的划分点。\n",
    "\n",
    "#### 信息增益率\n",
    "\n",
    "计算步骤如下：\n",
    "\n",
    "- 1.计算特征A相对于数据集D的信息增益$Gain(D,A)$\n",
    "\n",
    "\n",
    "- 2.计算特征A的熵：$H(A)=-\\sum_{a\\in A}p(a)*log[p(a)]$\n",
    "\n",
    "\n",
    "- 3.计算信息增益率：$GainRatio(D,A)=Gain(D,A)/H(A)$\n",
    "\n",
    "\n",
    "在选用信息增益率来为决策树选取划分结点时，**信息增益率最大**的特征将被视为当前的划分点。\n",
    "\n",
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:23.939074Z",
     "start_time": "2017-11-17T15:34:23.863068Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcInfoGain_or_InfoGainRate(dataSet, label, calcInfoGainRate=False):\n",
    "    '''计算数据集每一列（特征）的信息增益或信息增益率'''\n",
    "    def calcEntropy(data):\n",
    "        '''计算单列数据的熵'''\n",
    "        probs_ = np.array(list(Counter(data).values()))/data.shape[0]\n",
    "        ans = -1*(probs_*np.log2(probs_)).sum()\n",
    "        return ans\n",
    "    #计算数据集的熵\n",
    "    dataSetEntropy = calcEntropy(label)\n",
    "    #得到数据集的 样本数 和 特征数\n",
    "    sampleNum, featureNum = dataSet.shape\n",
    "    infoGains = np.zeros(featureNum) #用于保存 信息增益的数组\n",
    "    #对于数据集的每一个特征\n",
    "    for featureId in range(featureNum):\n",
    "        #得到当前的 特征\n",
    "        curFeature = dataSet[:, featureId]\n",
    "        #得到每个取值的统计次数\n",
    "        counter = Counter(curFeature)\n",
    "        #得到所有可能的取值\n",
    "        values = list(counter.keys())\n",
    "        #得到所有可能取值的概率\n",
    "        probs = np.array(list(counter.values()))/sampleNum\n",
    "        entropys = np.zeros(len(values)) #用于保存熵的数组\n",
    "        #遍历每个可能的取值\n",
    "        for index, val in enumerate(values):\n",
    "            #得到 标签 中对应的 子数据集标签\n",
    "            subLabel = label[np.argwhere(curFeature==val)[:,0]]\n",
    "            #计算 子数据集标签 的 熵\n",
    "            entropys[index] = calcEntropy(subLabel)\n",
    "        #计算基于当前特征的 条件熵\n",
    "        condEntropy = (probs*entropys).sum()\n",
    "        #计算基于当前特征的 信息增益\n",
    "        infoGains[featureId] = dataSetEntropy - condEntropy\n",
    "        #若是计算 信息增益率，则要除以 当前特征的 熵 \n",
    "        if calcInfoGainRate:\n",
    "            denominator = calcEntropy(curFeature)\n",
    "            #当 当前特征的 熵 为0时，信息增益也为0，此处避免除零错误\n",
    "            if denominator != 0:\n",
    "                infoGains[featureId] /= calcEntropy(curFeature)\n",
    "    return infoGains\n",
    "\n",
    "def calcInfoGain(dataSet, label):\n",
    "    '''计算数据集每一列（特征）的信息增益'''\n",
    "    return calcInfoGain_or_InfoGainRate(dataSet, label, calcInfoGainRate=False)\n",
    "\n",
    "def calcInfoGainRate(dataSet, label):\n",
    "    '''计算数据集每一列（特征）的信息增益率'''\n",
    "    return calcInfoGain_or_InfoGainRate(dataSet, label, calcInfoGainRate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试信息增益计算结果的正确性\n",
    "\n",
    "使用训练集进行测试，与其他同学的结果进行对比后，可知如下结果是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.067084Z",
     "start_time": "2017-11-17T15:34:23.939074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个特征的计算值如下：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09219297,  0.00969743,  0.0206067 ,  0.10927331,  0.00058833,\n",
       "        0.00121617,  0.00480776,  0.00819787,  0.01409991])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取特征的下标（从0开始）为： 3\n"
     ]
    }
   ],
   "source": [
    "ansA = calcInfoGain(trainSet, trainSet_label)\n",
    "print(\"每个特征的计算值如下：\")\n",
    "ansA\n",
    "print(\"选取特征的下标（从0开始）为：\", np.argmax(ansA) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试信息增益率计算结果的正确性\n",
    "\n",
    "使用训练集进行测试，与其他同学的结果进行对比后，可知如下结果是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.187092Z",
     "start_time": "2017-11-17T15:34:24.067084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个特征的计算值如下：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01876377,  0.00496779,  0.01309753,  0.03547937,  0.00103376,\n",
       "        0.00150852,  0.0030534 ,  0.00445081,  0.03086695])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取特征的下标（从0开始）为： 3\n"
     ]
    }
   ],
   "source": [
    "ansB = calcInfoGainRate(trainSet, trainSet_label)\n",
    "print(\"每个特征的计算值如下：\")\n",
    "ansB\n",
    "print(\"选取特征的下标（从0开始）为：\", np.argmax(ansB) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基尼指数\n",
    "\n",
    "### 实现原理\n",
    "\n",
    "计算公式为：$$Gini(D,A)=\\sum_{a \\in A}p(a)*gini(D|A=a)$$\n",
    "\n",
    "其中，在特征A取值为a时对应的数据集D的子集的基尼指数为：\n",
    "$$gini(D|A=a)=\\sum_{i=1}^np_i(1-p_i)=1-\\sum_{i=1}^np_i^2$$\n",
    "\n",
    "在选用基尼指数来为决策树选取划分结点时，**基尼指数最小**的特征将被视为当前的划分点。\n",
    "\n",
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.239095Z",
     "start_time": "2017-11-17T15:34:24.187092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcGiniIndex(dataSet, label):\n",
    "    '''计算数据集每一列（特征）的Gini指数'''\n",
    "    #得到数据集的 样本数 和 特征数\n",
    "    sampleNum, featureNum = dataSet.shape\n",
    "    giniIndexs = np.zeros(featureNum) #用于保存 信息增益的数组\n",
    "    #对于数据集的每一个特征\n",
    "    for featureId in range(featureNum):\n",
    "        #得到当前的 特征\n",
    "        curFeature = dataSet[:, featureId]\n",
    "        #得到每个取值的统计次数\n",
    "        counter = Counter(curFeature)\n",
    "        #得到所有可能的取值\n",
    "        values = list(counter.keys())\n",
    "        #得到所有可能取值的概率\n",
    "        probs = np.array(list(counter.values()))/sampleNum\n",
    "        subGiniIndexs = np.zeros(len(values)) #用于保存熵的数组\n",
    "        #遍历每个可能的取值\n",
    "        for index, val in enumerate(values):\n",
    "            #得到 标签 中对应的 子数据集\n",
    "            subLabel = label[np.argwhere(curFeature==val)[:,0]]\n",
    "            #计算 每个取值下 数据集的 gini指数\n",
    "            sub_values = np.array(list(Counter(subLabel).values()))\n",
    "            sub_probs = sub_values / subLabel.shape[0]\n",
    "            subGiniIndexs[index] = 1 - (sub_probs**2).sum()\n",
    "        #计算基于当前特征的 gini 指数\n",
    "        giniIndexs[featureId] = (probs*subGiniIndexs).sum()\n",
    "    return giniIndexs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试基尼指数计算结果的正确性\n",
    "\n",
    "使用训练集进行测试，与其他同学的结果进行对比后，可知如下结果是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.351103Z",
     "start_time": "2017-11-17T15:34:24.239095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个特征的计算值如下：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.43014533,  0.4774742 ,  0.47165246,  0.4254384 ,  0.48346148,\n",
       "        0.48304835,  0.48065305,  0.47857447,  0.47512049])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取特征的下标（从0开始）为： 4\n"
     ]
    }
   ],
   "source": [
    "ansC = calcGiniIndex(trainSet, trainSet_label)\n",
    "print(\"每个特征的计算值如下：\")\n",
    "ansC\n",
    "print(\"选取特征的下标（从0开始）为：\", np.argmax(ansC) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选取类实现\n",
    "\n",
    "为了更方便地使用3种特征选取，这里实现特征选取类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.463126Z",
     "start_time": "2017-11-17T15:34:24.351103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class featureSelection:\n",
    "    '''特征选取类：根据不同特征选取方法选取最优划分特征'''\n",
    "    \n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        \n",
    "    def getFeatureIndex(self, dataSet, label):\n",
    "        '''得到最优划分属性的下标（从0开始）'''\n",
    "        if self.method == 'ID3':\n",
    "            return np.argmax(calcInfoGain(dataSet, label))\n",
    "        elif self.method == 'C4.5':\n",
    "            return np.argmax(calcInfoGainRate(dataSet, label))\n",
    "        elif self.method == 'CART':\n",
    "            return np.argmin(calcGiniIndex(dataSet, label))\n",
    "        else:\n",
    "            print(\"ERROR: method not define!\")\n",
    "        \n",
    "##############测试程序###################\n",
    "ID3 = featureSelection('ID3')\n",
    "C45 = featureSelection('C4.5')\n",
    "CART = featureSelection('CART')\n",
    "\n",
    "a = ID3.getFeatureIndex(trainSet, trainSet_label)\n",
    "b = C45.getFeatureIndex(trainSet, trainSet_label)\n",
    "c = CART.getFeatureIndex(trainSet, trainSet_label)\n",
    "[a,b,c]\n",
    "##############测试程序###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树算法\n",
    "\n",
    "## 构建决策树的递归终止条件\n",
    "\n",
    "假设当前结点的数据集为D，特征集为A，递归终止条件及对应的处理如下：\n",
    "\n",
    "- 1.**D中的样本属于同一类别C，则将当前结点标记为C类叶结点**。\n",
    "    - 最简单的一种递归终止条件，很好理解。\n",
    "\n",
    "\n",
    "- 2.**A为空集，此时无法划分。将当前结点标记为叶结点，类别为D中出现最多的类**。\n",
    "    - 比如数据集为`[['a'],['a'],['a'],['b']]`，标签为`['yes','no','yes','no']`，则经过一次划分后，特征集就为空集了。因此便无法划分下去了。而对应分类为`'a'`的标签有`['yes','no','yes']`，因此便需选择出现最多的类为结果。\n",
    "\n",
    "\n",
    "- 3.**D中所有样本在A中所有特征上取值相同，此时无法划分。将当前结点标记为叶结点，类别为D中出现最多的类**。\n",
    "    -  比如数据集为`[['a', 'b'],['a', 'b'],['a', 'b']]`，标签为`['yes','no','yes']`。此时数据集的两个特征的各自的取值都是一样的，此时也无法划分，因此便需选择出现最多的类为结果。\n",
    "    \n",
    "   \n",
    "- 4.**D为空集，则将当前结点标记为叶结点，类别为父结点中出现最多的类**。\n",
    "    - 这种情况在本次报告暂时还没有出现过，因此在实现中忽略该条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法伪代码\n",
    "\n",
    "```\n",
    "buildTree(dataSet, label, featuresName)\n",
    "\n",
    "输入数据：\n",
    "\n",
    "-------------------------------------------------\n",
    "dataSet：训练数据集\n",
    "label：对应训数据集的标签\n",
    "featuresName：训练数据集的特征名称向量\n",
    "-------------------------------------------------\n",
    "\n",
    "输出数据：\n",
    "\n",
    "-------------------------------------------------\n",
    "tree:构建好的决策树\n",
    "-------------------------------------------------\n",
    "\n",
    "算法流程：\n",
    "-------------------------------------------------\n",
    "1.若dataSet中的样本属于同一类别C，则将当前结点标记为C类叶结点。停止递归。\n",
    "2.若dataSet为空集或dataSet中每个特征的取值都只有一个，则将当前结点标记为叶结点，类别为label中出现最多的类。停止递归。\n",
    "3.选择最优划分属性。\n",
    "4.以划分属性为结点构建一颗空树tree。\n",
    "5.存储该结点对应的最可能出现的标签值（用于预测未知值），使用label出现最多的标签值。\n",
    "6.对于划分属性的所有可能取值：\n",
    "6.1.根据划分属性当前取值，得到划分后的子数据集subDataSet和子标签subLabel。\n",
    "6.2.以buildTree(subDataSet, subLabel, featuresName)为分支结点。\n",
    "7.返回决策树tree。\n",
    "-------------------------------------------------\n",
    "```\n",
    "\n",
    "\n",
    "## 决策树类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.627192Z",
     "start_time": "2017-11-17T15:34:24.463126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decisionTree:\n",
    "    '''决策树类实现'''\n",
    "    \n",
    "    def __init__(self, method):\n",
    "        self.featureSelectionMethod = featureSelection(method=method) #特征选取方法\n",
    "    \n",
    "    def __getBestSplitFeature(self, dataSet, label):\n",
    "        '''得到数据集的最优划分属性的下标'''\n",
    "        return self.featureSelectionMethod.getFeatureIndex(dataSet, label)\n",
    "    \n",
    "    def __getSubSet(self, dataSet, label, splitIndex, splitValue):\n",
    "        '''根据划分属性的某个取值来得到对应的子数据集和标签'''\n",
    "        #得到对应取值的样本下标\n",
    "        sampleIndex = np.argwhere(dataSet[:, splitIndex]==splitValue)[:, 0]\n",
    "        #得到在原数据集的基础上删除划分属性所在列对应的样本\n",
    "        subDataSet = np.delete(dataSet, splitIndex, axis=1)[sampleIndex]\n",
    "        #得到子标签\n",
    "        subLabel = label[sampleIndex]\n",
    "        return subDataSet, subLabel\n",
    "    \n",
    "    def __getMostCommonLabel(self, label):\n",
    "        '''得到标签中出现最多次的数据'''\n",
    "        return Counter(label).most_common(1)[0][0]\n",
    "    \n",
    "    def __buildTree(self, dataSet, label, featuresName):\n",
    "        '''递归构建决策树'''\n",
    "        #递归终止条件1：若数据都属于一个类别，则返回该类别\n",
    "        if len(Counter(label)) == 1:\n",
    "            return label[0]\n",
    "        #递归终止条件2：若遍历完所有属性，则返回标签中出现次数最多的\n",
    "        if len(dataSet) == 0:\n",
    "            return self.__getMostCommonLabel(label)\n",
    "        #递归终止条件3：若所有样本在所有特征上取值相同，则返回标签中出现次数最多的\n",
    "        check = []\n",
    "        ## 遍历所有特征，得到每个特征的取值的次数\n",
    "        for featureIndex in range(dataSet.shape[1]):\n",
    "            check.append(len(Counter(dataSet[:, featureIndex])))\n",
    "        ## 若所有特征的取值都只有一个\n",
    "        if Counter(check).get(1,-1)==len(check):\n",
    "            return self.__getMostCommonLabel(label)\n",
    "        \n",
    "        #得到划分属性下标\n",
    "        splitIndex = self.__getBestSplitFeature(dataSet, label)\n",
    "        #得到划分属性\n",
    "        splitFeature = featuresName[splitIndex]\n",
    "        del(featuresName[splitIndex])\n",
    "        #以划分属性为结点构建一颗空树\n",
    "        tree = {splitFeature:{'tree':{}}} \n",
    "        #存储该结点对应的最可能出现的标签值（用于预测未知值）\n",
    "        tree[splitFeature]['defaultLabel'] = self.__getMostCommonLabel(label)\n",
    "        #遍历划分属性的所有可能取值\n",
    "        for val in set(dataSet[:, splitIndex]):\n",
    "            subDataSet, subLabel = self.__getSubSet(dataSet, label, splitIndex, val)\n",
    "            tree[splitFeature]['tree'][val] = self.__buildTree(subDataSet, subLabel, \n",
    "                                                             featuresName[:])\n",
    "        return tree\n",
    "        \n",
    "    def buildTree(self, dataSet, label, featuresName):\n",
    "        '''得到决策树'''\n",
    "        self.tree = self.__buildTree(dataSet, label, featuresName)\n",
    "    \n",
    "    def __apply(self, tree, sample, featuresName):\n",
    "        '''递归应用构建好的决策树对数据集进行分类'''\n",
    "        #得到根结点属性\n",
    "        rootFeature = list(tree.keys())[0] \n",
    "        #对应根结点的树\n",
    "        rootTree = tree[rootFeature]['tree']\n",
    "        #根据结点名称找到结点对应的下标\n",
    "        rootIndex = featuresName.index(rootFeature)\n",
    "        #遍历树的所有可能的分支\n",
    "        for val in rootTree.keys():\n",
    "            if sample[rootIndex] == val:\n",
    "                subTree = rootTree[val]\n",
    "                #若接下来是一棵树\n",
    "                if isinstance(subTree, dict):\n",
    "                    return self.__apply(subTree, sample, featuresName)\n",
    "                #若接下来是一个结点\n",
    "                else:\n",
    "                    return subTree\n",
    "        #若出现未知值，则返回默认的标签值\n",
    "        return tree[rootFeature]['defaultLabel']  \n",
    "            \n",
    "    def apply(self, dataSet, featuresName):\n",
    "        '''对数据集进行分类'''\n",
    "        ansLabel = np.zeros(dataSet.shape[0])\n",
    "        #遍历测试数据集的每一个样本\n",
    "        for index, sample in enumerate(dataSet):\n",
    "            ansLabel[index] = self.__apply(self.tree, sample, featuresName)\n",
    "        return ansLabel\n",
    "        \n",
    "    def getTree(self):\n",
    "        '''返回训练好的以字典形式存储的决策树'''\n",
    "        return self.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:24.867145Z",
     "start_time": "2017-11-17T15:34:24.627192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runDecisionTree(trainSet_, trainSet_label_, validateSet_, validateSet_label_):\n",
    "    '''运行决策树函数'''\n",
    "    def run(method):\n",
    "        test = decisionTree(method)\n",
    "        featuresName_ = list(range(trainSet_.shape[1]))\n",
    "        test.buildTree(trainSet_, trainSet_label_, featuresName_[:])\n",
    "        ans = test.apply(validateSet_, featuresName_[:])\n",
    "        diff = np.argwhere(ans == validateSet_label_)\n",
    "        accur = 100*float(diff.shape[0]/validateSet_label_.shape[0])\n",
    "        print(method+\":\", \"%.3f%%\" % accur)\n",
    "        \n",
    "    for method_ in ['ID3', 'C4.5', 'CART']:\n",
    "        run(method_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在原始训练集上运行决策树\n",
    "\n",
    "理论上来说，在原始数据集上运行决策树理应得到的准确率为100%，但数据集中可能存在着干扰，即是一些本来应该划分到同一个标签的具有高相似的数据的训练标签可能不一致，因此准确率会接近100%而有时不会等于100%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:25.860351Z",
     "start_time": "2017-11-17T15:34:24.871141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: 100.000%\n",
      "C4.5: 100.000%\n",
      "CART: 100.000%\n"
     ]
    }
   ],
   "source": [
    "runDecisionTree(trainSet_origin, trainSet_label_origin, \n",
    "                                        trainSet_origin, trainSet_label_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在随机划分的训练集和验证集上运行决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:26.344903Z",
     "start_time": "2017-11-17T15:34:25.860351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: 65.254%\n",
      "C4.5: 64.407%\n",
      "CART: 65.254%\n"
     ]
    }
   ],
   "source": [
    "runDecisionTree(trainSet, trainSet_label, validateSet, validateSet_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在手动划分的训练集和验证集上运行决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:26.845006Z",
     "start_time": "2017-11-17T15:34:26.344903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: 58.500%\n",
      "C4.5: 61.500%\n",
      "CART: 58.500%\n"
     ]
    }
   ],
   "source": [
    "runDecisionTree(trainSet2, trainSet2_label, validateSet2, validateSet2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较两种划分原始训练集的方式，随机划分得到的3种决策树的平均准确率要高于手动划分的。\n",
    "\n",
    "可能的原因应是：随机划分的训练集和验证集中的标签分布较为相似或接近，而手动划分的则没有这个效果。\n",
    "\n",
    "## 在测试集上应用决策树\n",
    "\n",
    "综合上面3种决策树的表现，这里选择使用随机划分数据得到的CART决策树来对测试集进行预测。\n",
    "\n",
    "### 得到最终预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:27.016999Z",
     "start_time": "2017-11-17T15:34:26.845006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1.0: 186, 1.0: 114})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTree = decisionTree('CART')\n",
    "featuresName_ = list(range(trainSet.shape[1]))\n",
    "testTree.buildTree(trainSet, trainSet_label, featuresName_[:])\n",
    "resLabel = testTree.apply(testSet, featuresName_[:])\n",
    "\n",
    "Counter(resLabel) #得到预测结果中标签的分布情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:27.037000Z",
     "start_time": "2017-11-17T15:34:27.016999Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('15352220_linzecheng.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in resLabel:\n",
    "        _ = f.write(str(int(i))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考题\n",
    "\n",
    "- 决策树有哪些避免过拟合的方法？\n",
    "    - 剪枝，比如预剪枝和后剪枝\n",
    "    - 使用多颗决策树，比如使用随机森林算法\n",
    "    - 其他方法：\n",
    "        - 限制树的深度\n",
    "        - 设置一个节点能被继续拆分出子节点所需要的包含最少的样本个数\n",
    "        - 设置最底层节点所需要包含的最少样本个数\n",
    "    \n",
    "    \n",
    "- C4.5相比于ID3的优点是什么？\n",
    "    - **ID3倾向于选择分支较多的属性**，而在大多数情况下，分支多的属性不一样是最有用的，比如数据集中每个样本都有一个唯一的id属性，若根据id属性进行划分，生成的树会显示非常庞大而低效。**C4.5在ID3的基础上除以特征的熵，这样就可以避免这个问题了**。这也就是C4.5的优点所在。\n",
    "    \n",
    "    \n",
    "- 如何用决策树来判断特征的重要性？\n",
    "    - 从直观上来讲，**判断一个属性重要不重要等价于判断其值的改变对结果的影响程度大不大**。\n",
    "    - 假设特征A在一颗决策树D上的结点的次数为M，在此次实验范围内，M为1。那么对于每个使用特征A划分出新分支的结点N，**可计算一种评测指标（比如说Gini指数）在该结点的变化值（用该结点的Gini指数和所有分支的Gini指数的总和的差来代表）来代表特征A在结点的重要性**。而将这M个值求平均就可得到特征A对整颗决策树的重要性了。\n",
    "    - 更进一步地，也可对多颗决策树来求平均得到特征对所有决策树的重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 友情测试\n",
    "\n",
    "帮舍友测试下他手动划分的训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T15:34:27.692243Z",
     "start_time": "2017-11-17T15:34:27.037000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 590\n",
      "label frequency: {1.0: 225, -1.0: 365}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 35.,   1.,   3., ...,   2.,   3.,   0.],\n",
       "       [ 24.,   1.,   2., ...,   2.,   2.,   0.],\n",
       "       [ 28.,   3.,   3., ...,   0.,   3.,   0.],\n",
       "       ..., \n",
       "       [ 36.,   1.,   2., ...,   1.,   2.,   0.],\n",
       "       [ 47.,   1.,   1., ...,   1.,   1.,   0.],\n",
       "       [ 30.,   1.,   2., ...,   1.,   0.,   0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 9\n",
      "number of sample in data : 197\n",
      "label frequency: {1.0: 98, -1.0: 99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 23.,   1.,   2., ...,   2.,   0.,   0.],\n",
       "       [ 36.,   1.,   1., ...,   2.,   3.,   0.],\n",
       "       [ 33.,   1.,   3., ...,   2.,   3.,   0.],\n",
       "       ..., \n",
       "       [ 26.,   3.,   3., ...,   2.,   2.,   0.],\n",
       "       [ 31.,   2.,   2., ...,   2.,   0.,   1.],\n",
       "       [ 32.,   0.,   2., ...,   2.,   2.,   0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: 58.376%\n",
      "C4.5: 60.914%\n",
      "CART: 58.376%\n"
     ]
    }
   ],
   "source": [
    "trainSet3, trainSet3_label = loadDataSet('.\\\\data\\\\train_2.csv')\n",
    "trainSet3\n",
    "validateSet3, validateSet3_label = loadDataSet('.\\\\data\\\\validation_2.csv')\n",
    "validateSet3\n",
    "runDecisionTree(trainSet3, trainSet3_label, validateSet3, validateSet3_label)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "595px",
    "left": "0px",
    "right": "915.844px",
    "top": "107px",
    "width": "358px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
