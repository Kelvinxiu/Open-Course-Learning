{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T17:19:35.324707Z",
     "start_time": "2017-11-21T17:19:30.598337Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import pdir as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集处理\n",
    "\n",
    "本次数据分为 train.csv 和 test.csv。每个文件有4列，前39列为特征，最后一列是标签（只有1和0两种取值）。\n",
    "\n",
    "## 数据集读取函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T17:19:35.346927Z",
     "start_time": "2017-11-21T17:19:35.324707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet(filePath):\n",
    "    ''' 数据集读取函数'''\n",
    "    data, label = [], []\n",
    "    # 读取数据集\n",
    "    with open(filePath) as f:\n",
    "        for line in f.readlines():\n",
    "            temp = line.strip().split(\",\")\n",
    "            data.append([float(i) for i in temp[:-1]])\n",
    "            if temp[-1] != '?':\n",
    "                temp[-1] = float(temp[-1])\n",
    "            label.append(temp[-1])\n",
    "    ##### 输出数据集相关信息 ##########\n",
    "    print(\"data dimension of dataset：\", len(data[0]))\n",
    "    print(\"number of sample in data :\", len(data))\n",
    "    print(\"label frequency:\", dict(Counter(label)))\n",
    "    ##### 输出数据集相关信息 ##########\n",
    "    return np.array(data), np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取原始训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T17:19:36.183036Z",
     "start_time": "2017-11-21T17:19:35.350912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 40\n",
      "number of sample in data : 8000\n",
      "label frequency: {0.0: 4040, 1.0: 3960}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.11, -0.45,  1.08, ..., -1.19, -0.63, -0.87],\n",
       "       [-0.05,  1.57,  1.66, ..., -0.25, -0.64, -0.16],\n",
       "       [-0.53,  0.71,  2.48, ...,  0.09, -2.33, -0.16],\n",
       "       ..., \n",
       "       [-0.05,  1.47, -0.1 , ..., -0.42,  2.3 ,  0.63],\n",
       "       [ 1.15, -1.71,  1.43, ...,  0.07, -0.35,  0.56],\n",
       "       [ 1.59, -0.68, -0.78, ...,  0.37,  0.02, -2.4 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet_origin, trainSet_label_origin = loadDataSet('.\\\\data\\\\train.csv')\n",
    "trainSet_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T17:19:36.411198Z",
     "start_time": "2017-11-21T17:19:36.191042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension of dataset： 40\n",
      "number of sample in data : 2000\n",
      "label frequency: {'?': 2000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68, -1.16,  0.83, ..., -0.08,  0.11,  0.12],\n",
       "       [-1.47, -0.3 ,  0.07, ..., -0.91, -0.31,  2.12],\n",
       "       [-1.94, -0.98,  0.12, ..., -0.11,  0.47, -0.78],\n",
       "       ..., \n",
       "       [-0.33, -0.79,  1.54, ..., -0.04, -1.43, -0.18],\n",
       "       [-0.65,  0.78, -0.58, ...,  1.32,  0.29,  0.61],\n",
       "       [-0.68,  1.15,  0.24, ..., -0.01, -0.9 , -1.1 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet, testSet_lable = loadDataSet('.\\\\data\\\\test.csv')\n",
    "testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从训练集中划分验证集\n",
    "\n",
    "典型的从训练集中划分验证集的方法是：划分训练集中的30%为验证集，划分过程采用随机选取的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T17:19:36.473260Z",
     "start_time": "2017-11-21T17:19:36.419204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分后的验证集和训练集的维度： (2400, 40) (5600, 40)\n"
     ]
    }
   ],
   "source": [
    "def splitTrainSet2ValidateSet(trainSet_, trainSet_label_, rate=0.3):\n",
    "    '''从训练集中划分验证集'''\n",
    "    #整合数据集和对应标签在同一个数组上，方便后续划分\n",
    "    allData = np.column_stack((trainSet_, trainSet_label_))\n",
    "    #随机打乱数据集\n",
    "    np.random.shuffle(allData)\n",
    "    #得到要划分的验证集的样本个数\n",
    "    splitNum = int(allData.shape[0]*rate)\n",
    "    #划分数据集\n",
    "    validateSet_ = allData[:splitNum, :-1]\n",
    "    validateSet_label_ = allData[:splitNum, -1]\n",
    "    trainSet_new = allData[splitNum:, :-1]\n",
    "    trainSet_label_new = allData[splitNum:, -1]\n",
    "    return trainSet_new, trainSet_label_new, validateSet_, validateSet_label_\n",
    "\n",
    "temp = splitTrainSet2ValidateSet(trainSet_origin, trainSet_label_origin)\n",
    "trainSet, trainSet_label, validateSet, validateSet_label = temp\n",
    "print('划分后的验证集和训练集的维度：', validateSet.shape, trainSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归算法\n",
    "\n",
    "## 算法原理\n",
    "\n",
    "逻辑回归是一种二分类算法，在本次实验中，分类标签便只有0和1两种取值。\n",
    "\n",
    "这里用 $y$ 来表示分类标签，则有 $y\\in\\{0,1\\}$。\n",
    "\n",
    "用  $X=[x_1,x_2,...,x_d]$ 来表示单个数据样本，其中 $x_i$ 是 $X$ 在第 $i$ 个维度的取值。\n",
    "\n",
    "逻辑回归的目标就是：建立一个模型，使得在给定数据 $X$ 的前提下，可以求该数据分类为 $y$ 的概率$P(y|X)$。\n",
    "\n",
    "逻辑回归借鉴了PLA的建模思路，对于数据集的d维特征，每个特征都有一个加分权重 $w_i$，因此总的加分值为\n",
    "\n",
    "$$score = \\sum_{i=1}^dw_i*x_i+\\theta$$\n",
    "\n",
    "其中，$\\theta$ 为阈值，为了方便，这里可令$\\theta=w_0$，因此可拓展上式为：\n",
    "\n",
    "$$score = \\sum_{i=0}^dw_i*x_i=W^TX$$\n",
    "\n",
    "这里$W^T=[w_0,w_1,...,w_d]$，$X$拓展为$X=[1, x_1,x_2,...,x_d]$\n",
    "\n",
    "而若要求解概率$P(y|X)$，显然需要找到一种方法将上面的加分值通过某种方式映射到$[0,1]$之间。\n",
    "\n",
    "这里逻辑回归便引入了sigmoid函数来实现这一目标，其定义为:$$\\theta(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "该函数的定义域为$[-\\infty ,+\\infty]$，值域为$(0,1)$，则：\n",
    "\n",
    "$$\\theta(score)=\\frac{1}{1+e^{-W^TX}}\\in (0,1)$$\n",
    "\n",
    "逻辑回归视 将由 $X$ 计算得来的加分值 代入上式后得到的值 为$X$属于正分类的概率：\n",
    "\n",
    "- 当$score \\rightarrow -\\infty,\\theta(score) \\rightarrow 0$，便认为$P(y=1|X)$为 0\n",
    "\n",
    "- 当$score =0,\\theta(score) =0.5$，便认为$P(y=1|X)$为0.5\n",
    "\n",
    "- 当$score \\rightarrow +\\infty,\\theta(score) \\rightarrow 1$，便认为$P(y=1|X)$为 1\n",
    "\n",
    "利用该函数，逻辑回归建立了如下的假说模型：$$h(X)=\\frac{1}{1+e^{-W^TX}}$$\n",
    "\n",
    "当$y=1$时，$P(y=1|X)=h(X)$；当$y=0$时，$P(y=0|X)=1-h(X)$；\n",
    "\n",
    "为了方便，可将上面的两种情况合并为一条式子：$$P(y|X,W)=h(x)^y(1-h(x))^{1-y}$$\n",
    "\n",
    "逻辑回归的目标便是要求出一个最理想的$W$，即：\n",
    "\n",
    "$$\\mathop{\\arg\\max}_{W} P(y|X,W)=\\mathop{\\arg\\max}_{W}h(x)^y(1-h(x))^{1-y}$$\n",
    "\n",
    "对于给定一个训练集$D={(X_1,y_1),(X_2,y_2),...,(X_n,y_n)}$，逻辑回归的优化目标则为：\n",
    "\n",
    "$$\n",
    "\\mathop{\\arg\\max}_{W} \\prod_{i=1}^n P(y_i|X_i,W)\n",
    "\\\\\\iff\n",
    "\\mathop{\\arg\\max}_{W}\\prod_{i=1}^n h(x)^y_i(1-h(x))^{1-y_i}\n",
    "\\\\\\iff\n",
    "\\mathop{\\arg\\min}_{W} -\\prod_{i=1}^n h(x)^y_i(1-h(x))^{1-y_i}\n",
    "\\\\\\iff\n",
    "\\mathop{\\arg\\min}_{W} -ln(\\prod_{i=1}^n h(x)^y_i(1-h(x))^{1-y_i})\n",
    "\\\\\\iff\n",
    "\\mathop{\\arg\\min}_{W} \\sum_{i=1}^n[y_i*ln(h(X_i)+(1-y_i)*ln(1-h(X_i))]\n",
    "$$\n",
    "\n",
    "注：这里为了求解方便，使用了对数变换，其不改变函数的极值点和最优解，并通过添加一个负号来将原来的最大化问题转换为最小化问题。\n",
    "\n",
    "令$Err(W)=\\sum_{i=1}^n[y_i*ln(h(X_i)+(1-y_i)*ln(1-h(X_i))]$\n",
    "\n",
    "可以证明，$Err(W)$是一个连续可导，二阶可微的凸函数，其存在全局最优解。\n",
    "\n",
    "而由于直接求解 $Err(W)$ 的最优解比较困难，故我们采取梯度下降法来求解其最优解。\n",
    "\n",
    "$Err(W)$ 的在第 $j$ 个维度的梯度求解后为:\n",
    "\n",
    "$$\\bigtriangledown Err(W_j)=\\sum_{i=1}^n[(\\frac{1}{1+e^{-W^TX}}-y_i)x_{i,j}]$$\n",
    "\n",
    "在求解了$Err(W)$在所有的维度的梯度之后，便可根据如下式子进行更新了。\n",
    "\n",
    "$$W_{t+1}=W_{t}-\\eta \\triangledown Err(W_t)$$\n",
    "\n",
    "其中，$\\eta$为人工设置的步长。\n",
    "\n",
    "算法伪代码见下部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法伪代码\n",
    "\n",
    "---\n",
    "\n",
    "**逻辑回归算法**\n",
    "\n",
    "---\n",
    "**输入**：参数W的给定初始值$W_0$、模型中的阈值$\\theta$、训练集$D={(X_1,y_1),(X_2,y_2),...,(X_n,y_n)}$、梯度下降的步长$\\eta$、算法中的迭代次数**maxRunTimes**\n",
    "\n",
    "**输出**：最优模型参数$W_{t+1}$\n",
    "\n",
    "---\n",
    "1:根据$W_0$和$\\theta$得到参数$W$的初始值\n",
    "\n",
    "2:给训练集 **D** 中的每一个样本前加一个常数1\n",
    "\n",
    "3:$for\\ t\\ =\\ 1,2,...,maxRunTimes\\ do$\n",
    "\n",
    "4:$\\ \\ \\ \\ \\ for\\ j\\ =\\ 1,2,...,d\\ do$\n",
    "\n",
    "5:$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $计算整个数据集在维度j的梯度：\n",
    "\n",
    "$$\\bigtriangledown Err(W_j)=\\sum_{i=1}^n[(\\frac{1}{1+e^{-W^TX}}-y_i)x_{i,j}]$$\n",
    "\n",
    "6:$\\ \\ \\ \\ \\ endfor$\n",
    "\n",
    "7:$\\ \\ \\ \\ \\ for\\ j\\ =\\ 1,2,...,d\\ do$\n",
    "\n",
    "8:$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $迭代更新$W$在维度j的取值：\n",
    "\n",
    "$$W_{t+1,j}=W_{t,j}-\\eta \\triangledown Err(W_{t,j})$$\n",
    "\n",
    "9:$\\ \\ \\ \\ \\ endfor$\n",
    "\n",
    "10:$endfor$\n",
    "更新预测错误的样本\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T23:00:05.057052Z",
     "start_time": "2017-11-21T23:00:05.001013Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''逻辑回归类实现'''\n",
    "    \n",
    "    def __addOne2Samples(self, dataSet):\n",
    "        '''给每一个样本前加一个常数1'''\n",
    "        ones = np.ones(len(dataSet))\n",
    "        return np.column_stack((ones, dataSet))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''sigmoid函数实现'''\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    \n",
    "    def fit(self, trainSet, label, eta=1e-3, maxRunTimes=100):\n",
    "        '''根据给定的训练集和标签训练PLA的参数 w '''\n",
    "        #设置默认的 w 全为1\n",
    "        self.w = np.mat(np.ones(trainSet.shape[1]+1)).reshape(-1,1)\n",
    "        #给训练集中每一个样本前加一个常数1，并转换为numpy矩阵\n",
    "        trainSet = np.mat(self.__addOne2Samples(trainSet))\n",
    "        #将标签转换为numpy矩阵，并将其设置为只有一列的数据的矩阵\n",
    "        label = np.mat(label).reshape(-1,1)\n",
    "        for i in range(maxRunTimes):\n",
    "            #根据矩阵运算得到整个数据集每个维度梯度\n",
    "            gradient = trainSet.transpose()*(self.sigmoid(trainSet*self.w) - label)\n",
    "            #print(\"第\"+str(i+1)+\"次迭代的梯度值：\\n\", gradient)\n",
    "            #更新 w\n",
    "            self.w -= eta*gradient\n",
    "        \n",
    "    def __apply(self, x):\n",
    "        '''利用训练好的 w 对输入的向量x进行分类'''\n",
    "        w = np.array(self.w)[:,0] #转换为numpy向量，方便后续计算\n",
    "        return 1 if self.sigmoid(np.dot(w, x)) > 0.5 else 0\n",
    "    \n",
    "    def apply(self, otherSet):\n",
    "        '''根据已训练出的 w 对其他数据集进行划分'''\n",
    "        otherSet = self.__addOne2Samples(otherSet)\n",
    "        outputLabel = np.zeros(otherSet.shape[0])\n",
    "        for index, sample in enumerate(otherSet):\n",
    "            outputLabel[index] = self.__apply(sample)\n",
    "        return outputLabel\n",
    "    \n",
    "    def getW(self):\n",
    "        return np.array(self.w)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化思路\n",
    "\n",
    "这里使用Numpy库的矩阵运算来实现算法的向量化运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T22:54:34.409805Z",
     "start_time": "2017-11-21T22:54:34.384789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代的梯度值：\n",
      " [[ 0.86281087]\n",
      " [ 1.74360795]\n",
      " [-0.9167695 ]]\n",
      "\n",
      "最终训练出来的 w 为： [ 0.13718913 -0.74360795  1.9167695 ]\n",
      "\n",
      "预测分类： [ 1.]\n"
     ]
    }
   ],
   "source": [
    "############测试程序###################\n",
    "c = [[1,2],[2,-1]] #trainset\n",
    "d = [1, 0]         #label\n",
    "e = [[3,3]]        #otherset\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(np.array(c), d, eta=1, maxRunTimes=1)\n",
    "print(\"\\n最终训练出来的 w 为：\", LR.getW())\n",
    "print(\"\\n预测分类：\", LR.apply(e))\n",
    "############测试程序###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "下面根据测试数据以及输出来验证算法实现的正确性。\n",
    "\n",
    "首先，给定的训练集为c，c中有两个样本数据：[1, 2]和[2, -1]，从d可知两者对应的分类标签为1和0。\n",
    "\n",
    "初始设置 $w_0$ 全为1，阈值 $\\theta$ 设置为1。\n",
    "\n",
    "那么在样本的第一个维度插入1后，即有：\n",
    "\n",
    "```\n",
    "***********************************************\n",
    "      训练集                      测试集          \n",
    "----------------------------   --------------  \n",
    "     样本           标签            样本\n",
    "x1 = [1,  1,  2]  |  y1 =  1   x3 = [1, 3, 3]\n",
    "x2 = [1,  2, -1]  |  y2 =  0\n",
    "\n",
    "算法参数 w 初始设置为： [1, 1, 1]\n",
    "***********************************************\n",
    "```\n",
    "\n",
    "这里 $\\eta$ 设置为1，迭代次数设置为1次。\n",
    "\n",
    "第一次迭代时，$w_0=[1, 1, 1]$，并且数据集的每个样本的权重分数为：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 2\\\\ \n",
    "1 & 2 & -1\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ \n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4\\\\ \n",
    "2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "即 $x_1$ 和 $x_2$ 各自的权重分数为 4 和 2。\n",
    "\n",
    "整个数据集的梯度为：\n",
    "\n",
    "$$\n",
    "gradient=\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "1 & 2\\\\\n",
    "2 & -1\n",
    "\\end{bmatrix}\n",
    "*(\n",
    "sigmoid\n",
    "(\\begin{bmatrix}\n",
    "4\\\\ \n",
    "2\n",
    "\\end{bmatrix})-\n",
    "\\begin{bmatrix}\n",
    "1\\\\ \n",
    "0\n",
    "\\end{bmatrix}\n",
    ")\\\\=\n",
    "\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "1 & 2\\\\\n",
    "2 & -1\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{-4}} -1\\\\ \n",
    "\\frac{1}{1+e^{-2}} -0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1*(\\frac{1}{1+e^{-4}} -1) + 1*(\\frac{1}{1+e^{-2}})\\\\ \n",
    "1*(\\frac{1}{1+e^{-4}} -1) + 2*(\\frac{1}{1+e^{-2}})\\\\\n",
    "2*(\\frac{1}{1+e^{-4}} -1) - 1*(\\frac{1}{1+e^{-2}})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.86281086801579077\\\\ \n",
    "1.7436079459936731\\\\\n",
    "-0.91676949790206541\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "该结果与上面程序的输出结果一致。此时的 $w_1$ 为：\n",
    "\n",
    "$$\n",
    "w_1=w_0-1*gradient\n",
    "\\\\=\\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "0.86281086801579077\\\\ \n",
    "1.7436079459936731\\\\\n",
    "-0.91676949790206541\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.13718913198420923\\\\\n",
    "-0.74360794599367308\\\\\n",
    "1.9167694979020653\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "计算结果与程序输出一致。\n",
    "\n",
    "计算 $w_1^T*x3$的值如下：\n",
    "\n",
    "$$\n",
    "w_1^T*x3=\n",
    "\\begin{bmatrix}\n",
    "0.13718913198420923 & -0.74360794599367308 & 1.9167694979020653\n",
    "\\end{bmatrix}*\\begin{bmatrix}\n",
    "1\\\\\n",
    "3\\\\\n",
    "3\n",
    "\\end{bmatrix}=3.6566737877093862\n",
    "$$\n",
    "\n",
    "代入sigmoid函数中有：\n",
    "\n",
    "$$\n",
    "\\frac{1}{1+e^{-w_1^T*x3}}=\\frac{1}{1+e^{-3.6566737877093862}}=0.97483155801980637>0.5\n",
    "$$\n",
    "\n",
    "因此，$x_3$应分类为 1，计算结果与程序输出一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  在验证集上应用算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T23:02:32.511404Z",
     "start_time": "2017-11-21T23:02:28.941520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 76.667%\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(trainSet, trainSet_label, eta=1e-5+1e-7, maxRunTimes=10000)\n",
    "ansLabel = LR.apply(validateSet)\n",
    "diff = np.argwhere(ansLabel == validateSet_label)\n",
    "accur = 100*float(diff.shape[0]/validateSet_label.shape[0])\n",
    "print(\"accuracy: %.3f%%\" % accur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在测试集上应用算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T23:31:42.689446Z",
     "start_time": "2017-11-21T23:31:38.357352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestLR = LogisticRegression()\n",
    "bestLR.fit(trainSet, trainSet_label, eta=1e-5+1e-7, maxRunTimes=10000)\n",
    "ansLabel = bestLR.apply(testSet)\n",
    "np.savetxt('15352220_linzecheng.txt', ansLabel, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考题\n",
    "\n",
    "- 1.如果把梯度为 0 作为算法停止的条件，可能存在怎样的弊端？\n",
    "    \n",
    "    - 答：可能会由于梯度始终都在 0 旁边波动而导致算法不会停止。\n",
    "    \n",
    "\n",
    "- 2.𝜂 的大小会怎么影响梯度下降的结果？给出具体的解释。\n",
    "\n",
    "    - 答：𝜂太小可能会导致梯度下降的过程显得比较缓慢；而𝜂太大可能会导致梯度下降的过程一致处于震荡状态，或导致迭代太快而错误最优解。\n",
    "    \n",
    "    \n",
    "- 3.解释批梯度下降和随机梯度下降的优缺点。\n",
    "\n",
    "    - 答：\n",
    "        - 从训练速度的角度来看：\n",
    "            - 随机梯度下降法每次仅仅采用一个样本来迭代，训练速度很快。\n",
    "            - 批量梯度下降法每次采用整个数据集来迭代，在样本量很大的时候，训练速度很慢。\n",
    "        - 从准确度的角度来看：\n",
    "            - 随机梯度下降法仅仅用一个样本决定梯度方向，导致解很有可能不是最优。\n",
    "            - 批量梯度下降法用整个数据集来决定梯度方向，得到的解相对较优。\n",
    "        - 从收敛速度的角度来看：\n",
    "            - 随机梯度下降法一次迭代只用一个样本，可能会导致迭代方向变化很大，不能很快地收敛到局部最优解。\n",
    "            - 批量梯度下降法每次采用整个数据集来迭代，迭代方向相对较稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 参考资料\n",
    "\n",
    "- [概率与似然][1]\n",
    "\n",
    "\n",
    "[1]:http://blog.csdn.net/fwing/article/details/4850068"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "704px",
    "left": "0px",
    "right": "930.25px",
    "top": "76px",
    "width": "402px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
