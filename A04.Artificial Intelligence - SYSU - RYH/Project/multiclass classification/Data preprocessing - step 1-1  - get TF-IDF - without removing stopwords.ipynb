{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.366Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import pdir as pr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "\n",
    "DF = pd.DataFrame\n",
    "arr = np.array\n",
    "\n",
    "import re\n",
    "import string\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.370Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadDataSet(filePath):\n",
    "    articles, labels = [], []\n",
    "    with open(filePath, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            part = line.split(\"\\t\")\n",
    "            #这里已经将标签转换为大写，数据转换成小写了\n",
    "            label, article = part[0].upper().strip(), part[2].lower().replace(\"<sssss>\", \" \").strip().split(\" \")\n",
    "            articles.append(article)\n",
    "            labels.append(label)\n",
    "    return articles, labels\n",
    "        \n",
    "\n",
    "train_articles, train_labels = loadDataSet(\".\\\\data\\\\MulLabelTrain.ss\")\n",
    "test_articles, test_labels = loadDataSet(\".\\\\data\\\\MulLabelTest.ss\")\n",
    "\n",
    "len(train_articles), len(test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.375Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_articles[0], \"\\n\\n\", train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除高频词和低频词得到所有词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.378Z"
    }
   },
   "outputs": [],
   "source": [
    "all_word_train_mix = [j for i in train_articles for j in i]\n",
    "all_word_test_mix = [j for i in test_articles for j in i]\n",
    "\n",
    "len(all_word_train_mix), len(all_word_test_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.382Z"
    }
   },
   "outputs": [],
   "source": [
    "all_word_mix = all_word_train_mix + all_word_test_mix\n",
    "len(all_word_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.390Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getCountDF(in_list):  \n",
    "    #根据Counter统计的词频初始化df\n",
    "    df = DF.from_dict(Counter(in_list), orient='index').reset_index()\n",
    "    #根据count值降序排序\n",
    "    df = df.rename(columns={'index':'words',\n",
    "                                  0:'count'}).sort_values([\"count\"],ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "all_word_mix_df = getCountDF(all_word_mix)\n",
    "all_word_mix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.396Z"
    }
   },
   "outputs": [],
   "source": [
    "low, high = 500, 50000\n",
    "df2 = all_word_mix_df[np.logical_and(low <= all_word_mix_df['count'], all_word_mix_df['count'] <= high)].reset_index(drop=True) \n",
    "df2.shape\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.400Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3 = df2.drop([\"count\"], axis=1)\n",
    "df3.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除数字以及其他无用词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.407Z"
    }
   },
   "outputs": [],
   "source": [
    "df3[\"words\"].shape\n",
    "df4 = df3[\"words\"].apply(lambda x: re.sub(\"[0-9||\\.||\\?||!]*\", \"\", x))\n",
    "df4 = df4[df4 != \"\"]\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.411Z"
    }
   },
   "outputs": [],
   "source": [
    "all_word_unique = df4.values\n",
    "all_word_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试-1\n",
    "\n",
    "检查训练集和测试集的词的不重复数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.415Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_word_train = set(all_word_train_mix)\n",
    "# all_word_test = set(all_word_test_mix)\n",
    "\n",
    "# len(all_word_train), len(all_word_test), (len(all_word_train) - len(all_word_test))\n",
    "\n",
    "# diff1 = all_word_train.difference(all_word_test)\n",
    "# diff2 = all_word_test.difference(all_word_train)\n",
    "\n",
    "# print(\"number of words in train but not in test: \", len(diff1))\n",
    "# print(\"number of words in test but not in train: \", len(diff2))\n",
    "\n",
    "# all_word = all_word_train.union(all_word_test)\n",
    "# len(all_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试-2\n",
    "检查词频分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.420Z"
    }
   },
   "outputs": [],
   "source": [
    "# def show_common_word_rate(lst, k):\n",
    "#     word_counter = Counter(lst)\n",
    "#     cnt = 0\n",
    "#     tot_cnt = len(lst)\n",
    "#     common_pair = word_counter.most_common(k)\n",
    "#     for key, val in common_pair:\n",
    "#         cnt += val\n",
    "#     print(\"max frequent word and count: \", common_pair[0])\n",
    "#     print(\"min frequent word and count: \", common_pair[-1])\n",
    "#     print(\"frequent word count: \", cnt)\n",
    "#     print(\"total word count: \", tot_cnt)\n",
    "#     print(\"remain word count: \", tot_cnt-cnt)\n",
    "#     print(\"rate:%.5f%%\" % (cnt/tot_cnt*100))\n",
    "\n",
    "# #测试\n",
    "# #show_common_word_rate([1,2,3,4,2,2], 2)\n",
    "# show_common_word_rate(all_word_mix, 500)\n",
    "# show_common_word_rate(all_word_mix, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到TF-IDF矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.425Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTF(dataSet, allWords):\n",
    "    '''得到输入数据集的TF矩阵'''\n",
    "    def safeDivide(a, b):\n",
    "        return a/b if b!=0 else 0\n",
    "    \n",
    "    TF=[]\n",
    "    for index in tnrange(len(dataSet)):\n",
    "        TF.append([])\n",
    "        wordCounter = Counter(dataSet[index])\n",
    "        for word in allWords:\n",
    "            TF[index].append(safeDivide(wordCounter.get(word,0), len(dataSet[index])))\n",
    "    return arr(TF)\n",
    "\n",
    "train_TF = getTF(train_articles, all_word_unique)\n",
    "test_TF = getTF(test_articles, all_word_unique)\n",
    "\n",
    "train_TF.shape, test_TF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存TF矩阵(**修改路径位置**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.429Z"
    }
   },
   "outputs": [],
   "source": [
    "dirPath = \"E:\\\\Code\\\\_largeData\\\\Github--Open-Course-Learning--A04\\\\Project\\\\multiclass classification\\\\data preprocessed\\\\tf-idf\"\n",
    "dirPath += \"\\\\without removing stopwords\"\n",
    "\n",
    "if not os.path.exists(dirPath):\n",
    "    os.makedirs(dirPath)\n",
    "\n",
    "    \n",
    "train_tf_DF = DF(train_TF)\n",
    "train_tf_DF.columns = all_word_unique\n",
    "\n",
    "test_tf_DF = DF(test_TF)\n",
    "test_tf_DF.columns = all_word_unique\n",
    "\n",
    "train_tf_DF.to_csv(dirPath + '\\\\train_tf.csv', index=False, header=True)\n",
    "test_tf_DF.to_csv(dirPath + '\\\\test_tf.csv', index=False, header=True)\n",
    "\n",
    "DF(all_word_unique).to_csv(dirPath + '\\\\all_word_unique.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意，有一些样本去掉停用词后整个都没有数据了，这时候就默认TF那一列都为0了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算TF-IDF矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.434Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getIDF(dataSet, allWords):\n",
    "    '''得到输入数据集的IDF矩阵'''\n",
    "    def calcIDF(num):\n",
    "        '''计算对应数据集的单词的IDF值'''\n",
    "        return math.log(len(dataSet)/(1+num), 2)\n",
    "    \n",
    "    IDF=[]\n",
    "    for i in tnrange(len(allWords)):\n",
    "        cnt = 0\n",
    "        #计算词在每个文档出现的次数\n",
    "        for doc in dataSet: \n",
    "            if allWords[i] in doc:\n",
    "                cnt += 1\n",
    "        IDF.append(calcIDF(cnt))\n",
    "    return arr(IDF)\n",
    "\n",
    "train_IDF = getIDF(train_articles, all_word_unique)\n",
    "test_IDF = getIDF(test_articles, all_word_unique)\n",
    "\n",
    "train_IDF.shape, test_IDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.440Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getIDF(dataSet, allWords):\n",
    "    '''得到输入数据集的IDF矩阵'''\n",
    "    def calcIDF(num):\n",
    "        '''计算对应数据集的单词的IDF值'''\n",
    "        return math.log(len(dataSet)/(1+num), 2)\n",
    "    \n",
    "    IDF=[]\n",
    "    for i in tnrange(len(allWords)):\n",
    "        cnt = 0\n",
    "        #计算词在每个文档出现的次数\n",
    "        for doc in dataSet: \n",
    "            if allWords[i] in doc:\n",
    "                cnt += 1\n",
    "        IDF.append(calcIDF(cnt))\n",
    "    return arr(IDF)\n",
    "\n",
    "train_IDF = getIDF(train_articles, all_word_unique)\n",
    "test_IDF = getIDF(test_articles, all_word_unique)\n",
    "\n",
    "train_IDF.shape, test_IDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.444Z"
    }
   },
   "outputs": [],
   "source": [
    "train_TFIDF = train_TF * train_IDF\n",
    "test_TFIDF = test_TF * test_IDF\n",
    "\n",
    "train_TFIDF.shape, test_TFIDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存TF-IDF矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.450Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_tfidf = DF(train_TFIDF)\n",
    "# train_tfidf.columns = all_word_unique\n",
    "\n",
    "# test_tfidf = DF(test_TFIDF)\n",
    "# test_tfidf.columns = all_word_unique\n",
    "\n",
    "# train_tfidf.to_csv(dirPath + '\\\\train_tfidf.csv', index=False, header=True)\n",
    "# test_tfidf.to_csv(dirPath + '\\\\test_tfidf.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.456Z"
    }
   },
   "outputs": [],
   "source": [
    "#划分比例\n",
    "splitRate = 0.3\n",
    "#划分的数目\n",
    "splitNum = int(train_TFIDF.shape[0]*splitRate) \n",
    "#得到 训练集 和验证集\n",
    "trainSet = train_TFIDF[:-splitNum]\n",
    "validateSet = train_TFIDF[-splitNum:]\n",
    "\n",
    "trainSet.shape, validateSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.459Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = arr(train_labels)\n",
    "test_labels = arr(test_labels)\n",
    "\n",
    "trainSetLabel = train_labels[:-splitNum]\n",
    "validateSetLabel = train_labels[-splitNum:]\n",
    "\n",
    "trainSetLabel.shape, validateSetLabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF(trainSetLabel[0:20]).replace(\"LOW\",0).replace(\"MID\",1).replace(\"HIG\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.466Z"
    }
   },
   "outputs": [],
   "source": [
    "DF(trainSet).to_csv(dirPath + '\\\\train.csv', index=False, header=False)\n",
    "DF(validateSet).to_csv(dirPath + '\\\\validate.csv', index=False, header=False)\n",
    "DF(test_TFIDF).to_csv(dirPath + '\\\\test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出标签映射为数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-08T10:50:55.469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSetLabel = DF(trainSetLabel).replace(\"LOW\",0).replace(\"MID\",1).replace(\"HIG\",2)\n",
    "validateSetLabel = DF(validateSetLabel).replace(\"LOW\",0).replace(\"MID\",1).replace(\"HIG\",2)\n",
    "\n",
    "DF(trainSetLabel).to_csv(dirPath + '\\\\train_label.csv', index=False, header=False)\n",
    "DF(validateSetLabel).to_csv(dirPath + '\\\\validate_label.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "937px",
    "left": "0px",
    "right": "1175px",
    "top": "111px",
    "width": "299px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "0549a248f09045d5992d795c3d9788cf": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "09353b0b60854243b180c8ee4f870265": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "3f334775746b4389b33b75687d120001": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "60cbed852de14cb7953b03896bfff7a0": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "651114c392a044b0b77021f7b39a85bd": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "7126a2acd4f34beb8e2185904b317c79": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "81144806500a4efa937d70324c425483": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "8133121273bc4d0fa878d353353cc55e": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "95313b98d38c47118b86112bf44c30a6": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    },
    "c6e5a454af6b47aa989cc975b0241733": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
