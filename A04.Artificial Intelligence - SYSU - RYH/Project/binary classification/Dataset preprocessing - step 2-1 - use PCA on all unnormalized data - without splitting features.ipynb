{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:00.221929Z",
     "start_time": "2017-12-25T09:22:00.207920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import pdir as pr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "\n",
    "DF = pd.DataFrame\n",
    "arr = np.array\n",
    "mat = np.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:03.826814Z",
     "start_time": "2017-12-25T09:22:00.223930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 18), (14400, 18), (12000, 17))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirPath = \"data preprocessed\\\\unnormalized\"\n",
    "\n",
    "trainSet_origin = np.loadtxt(dirPath + '\\\\train.csv', delimiter=\",\")\n",
    "validateSet_origin = np.loadtxt(dirPath + '\\\\validate.csv', delimiter=\",\")\n",
    "testSet = np.loadtxt(dirPath + '\\\\test.csv', delimiter=\",\")\n",
    "\n",
    "trainSet_origin.shape, validateSet_origin.shape, testSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:03.842826Z",
     "start_time": "2017-12-25T09:22:03.829816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet_merge = np.row_stack([trainSet_origin, validateSet_origin])\n",
    "trainSet_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:04.103620Z",
     "start_time": "2017-12-25T09:22:03.845327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet, trainSetLabel = trainSet_merge[:, :-1], trainSet_merge[:, -1]\n",
    "trainSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:15.363627Z",
     "start_time": "2017-12-25T09:22:15.342612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA(dataMat, topNfeat=9999999): \n",
    "    '''\n",
    "    输入：数据集，主成分数目\n",
    "    输出：包含数据的列表，reconmat\n",
    "    描述：降维后的数据集，保留的主成分所占的方差百分比\n",
    "    ''' \n",
    "    meanVals = np.mean(dataMat, axis=0) #计算数据的均值，axis=0代表按列求均值\n",
    "    meanRemoved = dataMat - meanVals    #将数据减去其原来的均值\n",
    "    covMat = np.cov(meanRemoved, rowvar=0)     #得到协方差矩阵，rowvar=0代表传入的数据一行代表一个样本\n",
    "                                               #（rowvar=1则代表传入的数据一列代表一个样本）\n",
    "    eigVals, eigVects = np.linalg.eig(mat(covMat)) #得到特征值和特征向量，两者一一对应\n",
    "    eigValIndex = np.argsort(eigVals)              #对特征值从小到大排列,得到排列后的下标\n",
    "                                                 #例：eigVals=[2.5,7.0,0.5],\n",
    "                                                 #则  eigValIndex=[1,2,0]\n",
    "    eigValIndex = eigValIndex[:-(topNfeat+1):-1] #逆序得到topNfeat个最大的特征值\n",
    "    redEigVects = eigVects[:,eigValIndex]        #得到topNfeat个最大的特征向量\n",
    "    lowDDataMat = meanRemoved * redEigVects      #将数据降到topNfeat维\n",
    "    varRate = np.sum(eigVals[eigValIndex])/np.sum(eigVals)*100 #得到保留的主成分的方差百分比\n",
    "    return lowDDataMat, varRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试主成分个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:22:23.925192Z",
     "start_time": "2017-12-25T09:22:23.241201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主成分        方差百分比      累积方差百分比\n",
      " 1               59.28             59.28\n",
      " 2               22.46             81.74\n",
      " 3                8.09             89.83\n",
      " 4                6.35             96.18\n",
      " 5                1.52             97.70\n",
      " 6                1.08             98.78\n",
      " 7                1.01             99.80\n",
      " 8                0.18             99.97\n",
      " 9                0.03             100.00\n",
      "10                0.00             100.00\n",
      "11                0.00             100.00\n",
      "12                0.00             100.00\n",
      "13                0.00             100.00\n",
      "14                0.00             100.00\n",
      "15                0.00             100.00\n",
      "16                0.00             100.00\n",
      "17                0.00             100.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totFeatureNum = trainSet.shape[1]\n",
    "preVarRate=0.0\n",
    "print(\"主成分        方差百分比      累积方差百分比\")\n",
    "for i in tnrange(1, totFeatureNum+1):\n",
    "        lowDDataMat, tolVarRate = PCA(mat(trainSet), i)           #主成分数目为i时的累积方差百分比\n",
    "        curVarRate = tolVarRate-preVarRate #当前新增加的主成分所占的方差百分比\n",
    "        print(\"%2d               %5.2f             %5.2f\"%(i, curVarRate, tolVarRate))\n",
    "        preVarRate = tolVarRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里发现，前9个成分保存了100%的信息。因此可以去降维到9而不损失信息。\n",
    "\n",
    "后续可以测试主成分为1-9的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试直接对step1的unnormalized数据进行PCA降维\n",
    "这里先忽略离散特征应和连续特征区分的“正确手段”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:24:36.908276Z",
     "start_time": "2017-12-25T09:24:36.903272Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirPath = \"data preprocessed\\\\unnormalized - use all data - use PCA\"\n",
    "if not os.path.exists(dirPath):\n",
    "    os.makesdir(dirPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先测试主成分为1-6的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T09:29:48.688175Z",
     "start_time": "2017-12-25T09:29:41.140848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in tnrange(1, 6+1):\n",
    "    fileName = '\\\\all-unnormalized-data-PCA=' + str(k) + '.csv'\n",
    "    lowDDataMat, _ = PCA(mat(trainSet), k)\n",
    "    DF(lowDDataMat).to_csv(dirPath + fileName, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "423d3d7c079644c4a3a187332ed1d857": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "4d60d3ac358c4be0a71acb9e9f00b435": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
